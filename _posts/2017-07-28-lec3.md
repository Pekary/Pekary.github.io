---
layout: post
title: CS231n笔记-Lec3 优化
date: 2017-07-28-12:00:00 +0800
author: Pekary
subtitle: Convolutional Neural Networks for Visual Recognition
tags: 深度学习
---

# CS231n笔记-Lec3 优化

## 1.梯度

- 对称（中心）差分公式：

  $$\frac{f(x+h) - f(x-h)}{2h}$$

  用这个式子来估计导数（梯度）比用定义来估计更精确。

- **Gradient check**

- **Mini-batch gradient descent(BGD)**: ILSVRC 120万张图像采用batch-size为256，通常采用32、64、128等2的幂，这样在向量化操作时效率更高。

- **Stochastic Gradient Descent (SGD)**: batch-size为1时候的BGD,通常不采用，因为对于向量化操作来说，计算一次100个样例的梯度比计算100次一个样例的梯度效率要高。

## 2.反向传播算法

本质上就是链式法则（Chain Rule），可以通过计算图来理解。

- 计算图（Computational Graph）:圆圈里的是运算符，最左端是输入，最右端是输出。其中**max**运算有一个非常有意思的理解方式，即理解为一个路由器选路，只将梯度传播给值大的那边。
- 缓存中间变量
- 在计算图中如果一个变量分支出了多个部分，计算梯度时需要累加。
- 如果在训练前**放大（缩小）输入值**，那么**权重值（参数值）**也会放大（缩小），相应地，我们需要降低（增大）学习率来作为补偿。
- 计算**矩阵梯度**技巧：$w$和$\mathrm{d} w$的维数应该一致。[矩阵向量求导指导](/assets/vecDerivs.pdf)