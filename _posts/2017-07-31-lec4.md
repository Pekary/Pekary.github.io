---
layout: post
title: CS231n笔记-Lec4 神经网络
author: Pekary
date: 2017-07-31-12:00:00 +0800
subtitle: Convolutional Neural Networks for Visual Recognition
tags: 深度学习
---

#  CS231n笔记 Lec4 神经网络

##1.神经网络模型

生物神经网络中树突(dendrite)作为输入与其他神经元的轴突相连，而轴突作为神经元的输出，人工神经网络有借鉴这个模型。神经网络有输入层、隐层和输出层。

- **人工神经网络仅仅是生物神经网络的一个粗糙模型**，比如生物神经网络会有多种类型的神经元，并且每个有着不同属性，树突进行复杂的非线性运算，突触是一个非线性动态系统，并且在精确的时间输出脉冲在许多系统中是重要的，这些在人工神经网络中只进行了简单的建模或者未建模。

- 单个神经元本质上是一个线性分类器（然后通过激活函数来加入非线性因素）。

- 其层数计算**不算输入层**，比如单层神经网络描述的是一个没有隐层的神经网络。

- 输出层通常不作用激活函数，因为输出层被用来计算分类得分，可以是任意实数。

- 计算神经网络训练的参数个数：

  ![Neural Nets](/assets/neural_net2.jpeg)

  该神经网络有4+4+1个神经元，权重参数有[3×4] + [4×4] + [4×1] = 32个，加上bias参数4+4+1个，共41个训练参数。

- **单隐层**神经网络可以逼近任意**连续**函数（[intuitive explanation](http://neuralnetworksanddeeplearning.com/chap4.html)），尽管该点在数学上可以证明，但是在实践中是一个相对薄弱或者说无用的条件。相关阅读：

  1）[Deep Learning](http://www.deeplearningbook.org/) book in press by Bengio, Goodfellow, Courville, in particular [Chapter 6.4](http://www.deeplearningbook.org/contents/mlp.html).

  2）[Do Deep Nets Really Need to be Deep?](http://arxiv.org/abs/1312.6184)

  3）[FitNets: Hints for Thin Deep Nets](http://arxiv.org/abs/1412.6550)

## 2.常见激活函数

- sigmoid函数：由于*1）*[严重]当激活函数输出值为0或者1时，会导致**梯度消失（Gradient Vanishing）**,实际上也可以发现sigmoid函数一阶导数的最大值为1/4,所以每经过一次激活函数梯度变为原来的1/4。*2）*sigmoid函数不是以0为中心的。由于输入数据为正数，sigmoid函数的值也为正数，所以梯度值要么都为正数要么都为负数，这将会导致优化路径为Z字型，效率低，不过这个问题可以通过累加一批次数据的梯度来缓和。

- tanh函数：这个函数同样存在梯度消失问题，实际上tanh函数是一个经过线性变换的sigmoid函数，存在关系$tanh(x) = 2\sigma(2x) - 1$ 。

- ReLU（Rectified Linear Unit）: $\max(0, x)$ ，这个函数近年来**常用**，有一些支持和反对意见。

  1) +  比起sigmoid和tanh，它被发现有利于加速SGD的收敛，论点是由于该函数是线性的且不饱和（non-saturating）。

  2) +  比sigmoid和tanh，ReLU计算开销更小。

  3) -   ReLU unit在训练过程中容易停滞（*Dying ReLU*），因为当一个较大梯度值通过ReLU神经元导致权重更新后，这个ReLU神经元将很难再被激活。这个问题可以通过**调整学习率**来缓和。

- Leaky ReLU：$f(x) = \mathbb{I} (x < 0)(\alpha x) + \mathbb{I} (x < 0)(x)$，这么设计的原因主要是为了解决Dying ReLU问题，不过这个函数并没有绝对改善。

- Maxout:  $\max (w_1^Tx + b_1, w_2^Tx + b_2)$，可以解决Dying ReLU和 saturating问题，但是由于要对每个神经元需要训练两组参数，使得参数数量增多。

**实践指导**：实践中同一个神经网络中混合多种神经元十分罕见，尽管这么做没有根本性问题，通常我们使用ReLU函数，如果在意Dying ReLU问题，尝试Leaky ReLU或者Maxout，永远不要使用sigmoid。

## 3.如何设置神经网络的层数以及每层神经元的个数

- 神经网络的层数和每层神经元的个数越多，神经网络的**表示能力**越强，但是同时会带来**过拟合**的问题，**实践中并不鼓励为了缓解过拟合问题而降低神经网络的规模，而是通过一些手段（L2 Regularization、Dropout等）来缓和**。
- 神经网络规模越小越难被训练，因为其容易陷入不好的局部最小值。